<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
    <title>Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:description" content="ADV-LLM: An iterative self-tuning approach to transform pretrained LLMs into adversarial models with enhanced jailbreak capabilities."/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet" href="bootstrap-grid.css">
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <script src="bootstrap.js"></script>
    <script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style type="text/css">
        body{font-family:Arial,Helvetica;font-weight:300;font-size:17px;margin:0 auto;}
        strong{font-weight:bold;}
        .container{margin:0 auto;padding:0 16px;}
        .paper-title h1{text-align:center;font-size:40px;font-weight:500;line-height:1.15em;padding:1px 0;}
        .author-row{text-align:center;font-size:26px;margin-top:10px;}
        .author-row a{display:inline-block;font-size:20px;padding:4px;text-decoration:none;color:#5364cc;}
        .affiliations,.venue{text-align:center;font-size:18px;margin-top:6px;}
        hr{height:1px;border:0;border-top:1px solid #ddd;margin:24px 0;}
        h2{font-size:1.75em;font-weight:400;margin:16px 0 4px;}
        h3{font-size:1.3em;margin:12px 0 4px;}
        p{text-align:justify;line-height:1.35em;}
        ul{margin:0 0 0 20px;padding:0;}
        li{margin-bottom:6px;line-height:1.35em;}
        .caption{font-size:14px;color:#565656;text-align:center;margin:10px 0;}
        .paper-btn{display:inline-flex;align-items:center;justify-content:center;gap:4px;margin:8px;padding:8px;width:120px;border:none;border-radius:2px;background:#5364cc;color:#fff;font-size:20px;font-weight:600;text-decoration:none;}
        .paper-btn:hover{opacity:0.85;}
        pre{font-size:14px;background:#eee;padding:16px;overflow-x:auto;}
        @media screen and (min-width:980px){body{width:980px;}}
    </style>
</head>

<body>
<div class="container">

<!-- Title & Authors -->
<div class="paper-title">
    <h1>Iterative Self-Tuning LLMs for<br>Enhanced Jailbreaking Capabilities</h1>
</div>

<div id="authors">
    <center>
        <div class="author-row">
            <a href="https://sunchungen.github.io/">Chung-En Sun<sup>1</sup></a>,
            <a href="https://www.microsoft.com/en-us/research/people/xiaodl/">Xiaodong Liu<sup>3</sup></a>,
            <a href="https://www.microsoft.com/en-us/research/people/weiwya/">Weiwei Yang<sup>3</sup></a>,
            <a href="https://lilywenglab.github.io/">Tsui-Wei Weng<sup>2</sup></a>,
            <a href="https://www.microsoft.com/en-us/research/people/chehao/">Hao Cheng<sup>3</sup></a>,
            <a href="https://aidansan.github.io/">Aidan San<sup>2</sup></a>,
            <a href="https://www.microsoft.com/en-us/research/people/mgalley/">Michel Galley<sup>3</sup></a>,
            <a href="https://www.microsoft.com/en-us/research/people/jfgao/">Jianfeng Gao<sup>3</sup></a>
        </div>
        <div class="affiliations">
            <span><sup>1</sup>University of California San Diego</span> | 
            <span><sup>2</sup>University of Virginia</span> | 
            <span><sup>3</sup>Microsoft Research</span>
        </div>
        <div class="venue"><b>NAACL 2025 (Oral)</b></div>

        <a class="paper-btn" href="https://arxiv.org/abs/2410.18469">
            <span class="material-icons">description</span>Paper
        </a>
        <a class="paper-btn" href="https://github.com/SunChungEn/ADV-LLM">
            <span class="material-icons">code</span>Code
        </a>
    </center>
</div>

<!-- Abstract -->
<section id="abstract">
    <hr><h2>Abstract</h2>
    <p>
        ADV-LLM is a self-tuning method that creates adversarial language models capable of highly effective jailbreak attacks. It achieves nearly 100% attack success rates (ASR) on open-source models like Llama2 and Llama3, with strong transferability to closed-source models (99% ASR on GPT-3.5, 49% on GPT-4). Compared to prior methods, ADV-LLM is more computationally efficient and also serves as a tool for advancing safety alignment research by generating valuable adversarial datasets.
    </p>
</section>

<!-- Motivation -->
<section id="motivation">
    <hr><h2>Motivation</h2>
	<p></p>
    <ul>
        <li>Ensuring the safety alignment of Large Language Models (LLMs) is increasingly critical as they are deployed in real-world applications.</li>
        <li>While automatic jailbreak attacks have emerged as a way to test model robustness, existing techniques are either inefficient or ineffective against well-aligned models like Llama2 and Llama3.</li>
        <li>Moreover, these methods rely heavily on expensive optimization procedures or pre-generated datasets, limiting their scalability and real-world applicability.</li>
        <li><strong>Goal:</strong> Develop an efficient, scalable, and highly effective method for crafting jailbreak prompts—one that can improve attack success rates with minimal computational cost and reveal deeper insights into LLM safety vulnerabilities.</li>
    </ul>
    <center>
        <img class="card-img-top" src="assets/comparison.png" style="width:900px" alt="Figure placeholder">
        <p class="caption"><i>Comparison of jailbreak methods.</i></p>
    </center>
</section>

<!-- ADV-LLM Framework -->
<section id="methodology">
    <hr><h2>ADV-LLM: Iterative Self-Tuning Framework</h2>
	<center>
        <img class="card-img-top" src="assets/pipeline.png" style="width:900px" alt="Overview figure">
        <p class="caption"><i>The pipline of ADV-LLM from training to testing. It begins by designing a better starting point, followed by iterative self-tuning to progressively improve jailbreak ability. Once trained, ADV-LLM acts as an adversarial attacker that generates effective suffixes for any given harmful query.</i></p>
    </center>
	<p>
		ADV-LLM is a novel iterative self-tuning framework that transforms a standard pretrained LLM into an adversarial suffix generator capable of bypassing safety alignment. The method begins with carefully designed starting point (<strong>suffix initialization</strong> and <strong>target refinement</strong>), reducing the search space and improving the likelihood of successful jailbreaks.</li>
	</p>
	<center>
        <img class="card-img-top" src="assets/suffix_init_target_refine.png" style="width:700px" alt="Overview figure">
        <p class="caption"><i>We initialize a starting suffix and refine the target to make the jailbreaking easier.</i></p>
    </center>
	<p>
		Once we have a better starting point. ADV-LLM iteratively undergoes two phase to fine-tunes itself:
	</p>
    <ul>
		<li><em><strong>Suffix Sampling</strong> (Phase 1):</em> The model generates candidate suffixes for a given harmful query and selects successful ones based on whether they elicit unsafe responses from a target victim model.</li>
		<li><em><strong>Knowledge Updating</strong> (Phase 2):</em> The model is fine-tuned on the accumulated successful suffixes.</li>
	</ul>
	<p>
		This process gradually improves the model’s ability to generate effective adversarial suffixes without relying on external data or gradient information from the target model.</li>
	</p>
    <center>
        <img class="card-img-top" src="assets/selftuning.png" style="width:700px" alt="Overview figure">
        <p class="caption"><i>ADV-LLM iteratively generates data for self-tuning.</i></p>
    </center>
</section>

<!-- Experiments -->
<section id="experiments">
    <hr><h2>Experiments and Results</h2>
	<p></p>
    <!-- Settings -->
    <h3>Settings</h3>
	<p></p>
    <ul>
        <li><strong>Dataset:</strong> 520 harmful queries from AdvBench.</li>
        <li><strong>Models:</strong> Vicuna, Guanaco, Mistral, Llama2, and Llama3.</li>
        <li><strong>Evaluation Metrics:</strong> 
            <ul>
                <li><em>Template Check</em> – detects refusal patterns such as “I cannot” or “As an AI”.</li>
                <li><em>LlamaGuard Check</em> – a safety classifier that flags unsafe content in model outputs.</li>
                <li><em>GPT-4 Check</em> – determines whether the response is both harmful and provides a detailed, actionable answer.</li>
            </ul>
            A successful attack must bypass all three checks to be considered fully effective. Results are reported as <code>{Template}/{LlamaGuard}/{GPT-4} %</code>.
        </li>
        <li><strong>Baselines:</strong> Search-based methods (GCG, I-GCG, AutoDAN, Cold-Attack, Beast) and LLM-based methods (AmpleGCG).</li>
    </ul>

    <!-- Experiment 1 -->
    <h3>Experiment 1 – Attack Success Rate</h3>
    <p>
        ADV-LLM significantly outperforms all baselines. With just one attempt per query (greedy decoding), it achieves higher attack success rates across all models. Using 50 attempts (group beam search), it reaches nearly <strong>100% ASR</strong> on all open-source models.
    </p>
    <center>
        <img class="card-img-top" src="assets/asr_search_based.png" style="width:900px" alt="ASR search-based">
        <p class="caption"><i>Table 1: The ASR of ADV-LLMs compared with search-based methods.</i></p>
    </center>
    <center>
        <img class="card-img-top" src="assets/asr_generation_based.png" style="width:900px" alt="ASR generation-based">
        <p class="caption"><i>Table 2: The ASR of ADV-LLMs compared with LLM-based methods.</i></p>
    </center>

    <!-- Experiment 2 -->
    <h3>Experiment 2 – Transferability</h3>
    <p>
        We test whether ADV-LLM’s attacks extend to closed-source models like GPT-3.5 and GPT-4. Even though it is trained solely on Llama3, ADV-LLM achieves <strong>99% ASR on GPT-3.5</strong> and <strong>49% ASR on GPT-4</strong>, indicating strong cross-model transferability—an essential property for real-world jailbreak evaluations.
    </p>
    <center>
        <img class="card-img-top" src="assets/transferability.png" style="width:900px" alt="Transferability figure">
        <p class="caption"><i>Table 3: The transferability of ADV-LLMs.</i></p>
    </center>

    <!-- Experiment 3 -->
    <h3>Experiment 3 – OOD Generalizability</h3>
    <p>
        To assess how well ADV-LLM handles unseen queries, we evaluate it on <em>MaliciousInstruct</em>. Despite being trained only on AdvBench, ADV-LLM achieves strong performance on this unseen set, showing that it generalizes effectively to new and diverse harmful query formats.
    </p>
    <center>
        <img class="card-img-top" src="assets/generalizability.png" style="width:900px" alt="Generalizability figure">
        <p class="caption"><i>Table 4: The generalizability of ADV-LLMs.</i></p>
    </center>

    <!-- Experiment 4 -->
    <h3>Experiment 4 – Stealthiness</h3>
    <p>
        To measure stealthiness, we evaluate whether the generated suffixes can evade <em>perplexity-based defenses</em>, which flag unnatural or low-fluency inputs. ADV-LLM consistently produces suffixes with lower perplexity, remaining effective even without repetition tricks.
    </p>
    <center>
        <img class="card-img-top" src="assets/stealthiness.png" style="width:800px" alt="Stealthiness figure">
        <p class="caption"><i>Table 5: Perplexity and ASR against perplexity defense of ADV-LLMs compared with AmpleGCGs.</i></p>
    </center>
</section>

<!-- Conclusion -->
<section id="conclusion">
	<p></p>
    <hr><h2>Conclusion</h2>
    <ul>
        <li>ADV-LLM achieves <strong>≈100% ASR</strong> on Llama2 and Llama3.</li>
        <li>It transfers effectively to closed-source models, reaching <strong>99% ASR</strong> on GPT-3.5 and <strong>49% ASR</strong> on GPT-4.</li>
        <li>It generalizes well to unseen datasets such as <em>MaliciousInstruct</em>.</li>
        <li>Generated suffixes have lower perplexity and successfully bypass perplexity-based defenses.</li>
        <li>The framework is compute-efficient and scalable, providing a valuable tool for probing and improving LLM safety alignment.</li>
    </ul>
</section>

<!-- References -->
<section id="references">
    <hr><h2>References</h2>
    <p><strong>[GCG]</strong> Zou et al. (2023). <a href="https://arxiv.org/abs/2307.15043" target="_blank"><em>Universal and Transferable Adversarial Attacks on Aligned Language Models</em></a>.</p>
    <p><strong>[I-GCG]</strong> Jia et al. (2024). <a href="https://arxiv.org/abs/2405.21018" target="_blank"><em>Improved Techniques for Optimization-Based Jailbreaking on Large Language Models</em></a>.</p>
    <p><strong>[AutoDAN]</strong> Liu et al. (2023). <a href="https://arxiv.org/abs/2310.04451" target="_blank"><em>AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models</em></a>.</p>
    <p><strong>[COLD-Attack]</strong> Guo et al. (2024). <a href="https://arxiv.org/abs/2402.08679" target="_blank"><em>COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability</em></a>.</p>
    <p><strong>[BEAST]</strong> Sadasivan et al. (2024). <a href="https://arxiv.org/abs/2402.15570" target="_blank"><em>Fast Adversarial Attacks on Language Models In One GPU Minute</em></a>.</p>
    <p><strong>[AmpleGCG]</strong> Liao and Sun (2024). <a href="https://arxiv.org/abs/2404.07921" target="_blank"><em>AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs</em></a>.</p>
</section>

<!-- Citation -->
<section id="citation">
    <hr><h2>Cite this Work</h2>
    <p>
        Chung-En Sun, Xiaodong Liu, Weiwei Yang, Tsui-Wei Weng, Hao Cheng, Aidan San, Michel Galley, and Jianfeng Gao.
        <b>"Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities."</b> <i>NAACL 2025</i>.
    </p>
    <pre><code>@article{advllm,
  title  = {Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities},
  author = {Chung-En Sun, Xiaodong Liu, Weiwei Yang, Tsui-Wei Weng, Hao Cheng, Aidan San, Michel Galley, Jianfeng Gao},
  journal= {NAACL},
  year   = {2025}
}</code></pre>
</section>

<!-- Footer -->
<section id="footer">
    <hr>
    <p style="text-align:center;">
        This webpage template was adapted from a previous template.<br>
        <a href="https://accessibility.ucsd.edu/" style="color:#5364cc"><b>Accessibility</b></a>
    </p>
</section>

</div>
</body>
</html>
